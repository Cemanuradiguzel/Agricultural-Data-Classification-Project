# -*- coding: utf-8 -*-
"""SoilType.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ip2xMHYOFhTgoPpFoVDtPOqk6Yd6P8rQ
"""

import numpy as np
import pandas as pd
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import MinMaxScaler

df=pd.read_excel('Data_processed.xlsx')
df_=pd.read_excel('Original.xls')

df

missing_values = df.isnull().sum()

missing_categories = missing_values[missing_values > 0]

print("Boş değerleri olan kategoriler:\n", missing_categories)

df['Longitude'] = df['Longitude'].astype(str)
df['Longitude'] = df['Longitude'].str.replace(r'[^\d\.\-]', '', regex=True)
df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')
mean_longitude = df['Longitude'].mean()
df['Longitude'].fillna(mean_longitude, inplace=True)

missing_values = df.isnull().sum()

missing_categories = missing_values[missing_values > 0]

print("Boş değerleri olan kategoriler:\n", missing_categories)

missing_variables = ["HerbicideYear","HerbicideMonth", "HerbicideWeekNum"]
missing_columns_in_dataset = [col for col in df.columns if col in missing_variables]

for col in missing_columns_in_dataset:
    df[col] = df[col].fillna(method='ffill')

missing1_variables =["HerbicideDay","DaysFromSowingToHerbicide","DaysFromHerbicideToHarvest"]
missing1_columns_in_dataset = [col for col in df.columns if col in missing1_variables]

imputer = KNNImputer(n_neighbors=5)
df[missing1_columns_in_dataset] = imputer.fit_transform(df[missing1_columns_in_dataset])

for col in missing1_columns_in_dataset:
    df[col] = pd.to_numeric(df[col], errors='coerce')
    df[col] = df[col].fillna(0).astype(int)

missing_values = df.isnull().sum()

missing_categories = missing_values[missing_values > 0]

print("Boş değerleri olan kategoriler:\n", missing_categories)

df_['SoilType'] = df_['SoilType'].astype(str)
df_['SoilType'].hist()

df['GrainYield']=df['GrainYield'].map({'A':0,'B':1,'C':2})
Y=df['GrainYield'].to_numpy()

for col in df.columns:
        df[col] = pd.to_numeric(df[col], errors='coerce')

df['PrevCropResidue']

scaler = MinMaxScaler() #performans artımmak için her şeyi numeric yaptık ve 0 1 arasına normalize ettik

columns_to_scale = df.columns[df.columns.get_loc('Longitude'):df.columns.get_loc('DaysFromHerbicideToHarvest') + 1].tolist()


df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])

df_heavy=df[df["SoilType_Heavy"]==1].copy()

df_heavy=df_heavy.drop(columns=['SoilType_Heavy','SoilType_Low','SoilType_Medium'])

df_heavy

df_low=df[df["SoilType_Low"]==1].copy()
df_low=df_low.drop(columns=['SoilType_Heavy','SoilType_Low','SoilType_Medium'])

df_low

df_medium=df[df["SoilType_Medium"]==1].copy()
df_medium=df_medium.drop(columns=['SoilType_Heavy','SoilType_Low','SoilType_Medium'])

df_medium

missing_values = df_medium.isnull().sum()

missing_categories = missing_values[missing_values > 0]

print("Boş değerleri olan kategoriler:\n", missing_categories)

!pip install catboost

from sklearn.linear_model import LogisticRegression


X = df_heavy[['Longitude', 'Latitude','Split1Urea','Split2Urea','Split3Urea']]
y = df_heavy['GrainYield']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=53)


model = LogisticRegression(max_iter=2000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)


accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

df_heavy['GrainYield'].value_counts()

import warnings
warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", category=FutureWarning, module="sklearn")

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.ensemble import AdaBoostClassifier, VotingClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import BernoulliNB, MultinomialNB
from catboost import CatBoostClassifier
import numpy as np
from sklearn.feature_selection import RFE, SelectKBest, mutual_info_classif
from sklearn.calibration import CalibratedClassifierCV
from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score, matthews_corrcoef
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from imblearn.over_sampling import SMOTE
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import accuracy_score
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import RidgeClassifier, ElasticNet, SGDClassifier
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import RANSACRegressor
from sklearn.metrics import (
    roc_curve, roc_auc_score, auc, confusion_matrix, ConfusionMatrixDisplay,
    accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef
)
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import label_binarize

models = [
    ('Logistic Regression', LogisticRegression(class_weight='balanced', solver='liblinear', random_state=53), {
        'C': [0.1, 1.0, 10],
        'max_iter': [1000, 2000]
    }),

    ('K-Nearest Neighbors (KNN)', KNeighborsClassifier(), {
        'n_neighbors': [3, 5, 7],
    }),

    ('Linear SVC', SVC(kernel='linear', class_weight='balanced', random_state=53), {
        'C': [0.1, 1.0, 10],
        'max_iter': [1000, 2000]
    }),

    ('Decision Tree Classifier', DecisionTreeClassifier(class_weight='balanced', random_state=53), {
        'max_depth': [5, 10, 15],
        'min_samples_split': [2, 5, 10]
    }),

    ('Logistic Regression with L1 Regularization', LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', random_state=53), {
        'C': [0.1, 1.0, 10]
    }),

    ('Extra Trees Classifier', ExtraTreesClassifier(random_state=53), {
        'n_estimators': [50, 100],
        'max_depth': [5, 10, 15],
        'min_samples_split': [2, 5, 10]
    }),

    ('AdaBoost Classifier', AdaBoostClassifier(random_state=53), {
        'n_estimators': [50, 100, 200],
        'learning_rate': [0.1, 1.0]
    }),

    ('Voting Estimators', VotingClassifier(estimators=[('knn', KNeighborsClassifier(n_neighbors=3)),
                                                      ('et', ExtraTreesClassifier(n_estimators=50, random_state=53))], voting='soft'), {
        'voting': ['hard', 'soft']
    }),

    ('Calibrated Classifier CV', CalibratedClassifierCV(estimator=LogisticRegression(max_iter=4000, class_weight='balanced', random_state=53), method='sigmoid'), {
        'estimator__C': [0.1, 1.0, 10],
        'estimator__max_iter': [1000, 2000, 4000]
    }),

    ('Bernoulli Naive Bayes', BernoulliNB(alpha=1.0), {
        'alpha': [0.5, 1.0, 2.0]
    }),

    ('Multinomial Naive Bayes', MultinomialNB(), {
        'alpha': [0.5, 1.0, 2.0]
    }),

    ('Passive-Aggressive Classifier', PassiveAggressiveClassifier(max_iter=2000, random_state=53), {
        'C': [0.1, 1.0, 10],
        'max_iter': [1000, 2000, 5000]
    }),('Ridge Classifier', RidgeClassifier(random_state=53), {
        'alpha': [0.1, 1.0, 10]
    }),

    ('ElasticNet Classifier', ElasticNet(random_state=53), {
        'alpha': [0.1, 1.0, 10],
        'l1_ratio': [0.1, 0.5, 0.9]
    }),

    ('SGD Classifier', SGDClassifier(random_state=53), {
        'loss': ['hinge', 'log', 'modified_huber'],
        'penalty': ['l2', 'l1', 'elasticnet'],
        'alpha': [0.0001, 0.001, 0.01]
    }),

    ('Histogram-based Gradient Boosting Classifier', HistGradientBoostingClassifier(random_state=53), {
        'max_iter': [50, 100],
        'learning_rate': [0.01, 0.1, 0.5],
        'max_depth': [3, 5, 7]
    }),


    ('Quadratic Discriminant Analysis', QuadraticDiscriminantAnalysis(), {
        'reg_param': [0.0, 0.1, 1.0],
        'priors': [None, [0.5, 0.5]]
    }),

    ('Gaussian Naive Bayes', GaussianNB(), {
        'var_smoothing': [1e-9, 1e-8, 1e-7]
    }),

    ('RANSAC Regressor', RANSACRegressor(random_state=53), {
        'residual_threshold': [0.5, 1.0, 5.0],
        'max_trials': [100, 200],
    }),
]

from collections import Counter

def evaluate_models(X, y, models, test_size=0.2, random_state=53, feature_count=30, min_score_threshold=0.70):


    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)


    selector = SelectKBest(mutual_info_classif, k=feature_count)
    X_train_selected = selector.fit_transform(X_train, y_train)
    X_test_selected = selector.transform(X_test)


    smote = SMOTE(random_state=random_state)
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)
    results = []
    all_selected_features = []

    for name, model, param_grid in models:
        print(f"Evaluating {name}...")
        grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', verbose=1, n_jobs=-1)
        best_score_overall = -np.inf

        for train_idx, val_idx in cv.split(X_train_selected, y_train):
            X_train_fold, X_val_fold = X_train_selected[train_idx], X_train_selected[val_idx]
            y_train_fold, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]


            X_train_resampled, y_train_resampled = smote.fit_resample(X_train_fold, y_train_fold)


            grid_search.fit(X_train_resampled, y_train_resampled)
            best_score = grid_search.best_score_

            if best_score > best_score_overall:
                best_score_overall = best_score
                best_model = grid_search.best_estimator_

        if best_score_overall >= min_score_threshold:
            y_pred = best_model.predict(X_test_selected)

            test_accuracy = accuracy_score(y_test, y_pred)
            auc_score = roc_auc_score(y_test, best_model.predict_proba(X_test_selected), multi_class='ovr', average='weighted') if hasattr(best_model, 'predict_proba') else None
            f1 = f1_score(y_test, y_pred, average='weighted')
            precision = precision_score(y_test, y_pred, average='weighted')
            recall = recall_score(y_test, y_pred, average='weighted')
            mcc = matthews_corrcoef(y_test, y_pred)

            selected_feature_names=X.columns[selector.get_support(indices=True)]

            all_selected_features.extend(selected_feature_names)



            results.append({
                'Model': name,
                'Best Score (Overall)': best_score_overall,
                'Test Accuracy': test_accuracy,
                'AUC': auc_score,
                'F1 Score': f1,
                'Precision': precision,
                'Recall': recall,
                'MCC': mcc,
                'Selected Features': selected_feature_names,
                'Best Model': best_model,
            })

    most_common_features = Counter(all_selected_features).most_common(5)
    print("\nTop 5 most common features across all models:")

    for feature, count in most_common_features:
        print(f"{feature}: {count} times")

    for result in results:
        print(result)

    plt.figure(figsize=(12, 8))
    for result in results:
        model_name = result["Model"]
        best_model = result["Best Model"]

        if hasattr(best_model, "predict_proba"):
            y_score = best_model.predict_proba(X_test_selected)
        elif hasattr(best_model, "decision_function"):
            y_score = best_model.decision_function(X_test_selected)
        else:
            print(f"{model_name} does not support ROC curve.")
            continue

        y_test_binarized = label_binarize(y_test, classes=np.unique(y_test))
        fpr, tpr, roc_auc = {}, {}, {}

        for i in range(y_test_binarized.shape[1]):
            fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], y_score[:, i])
            roc_auc[i] = auc(fpr[i], tpr[i])

        fpr_micro, tpr_micro, _ = roc_curve(y_test_binarized.ravel(), y_score.ravel())
        roc_auc_micro = auc(fpr_micro, tpr_micro)
        plt.plot(fpr_micro, tpr_micro, label=f"{model_name} (AUC = {roc_auc_micro:.2f})")

    plt.plot([0, 1], [0, 1], "k--", label="Chance")
    plt.xlabel("False Positive Rate")
    plt.ylabel("True Positive Rate")
    plt.title("ROC Curves for All Algorithms")
    plt.legend(loc="lower right")
    plt.show()


    best_result = max(results, key=lambda x: x["Best Score (Overall)"])
    best_model = best_result["Best Model"]

    y_pred = best_model.predict(X_test_selected)
    conf_matrix = confusion_matrix(y_test, y_pred)

    disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.unique(y_test))
    disp.plot(cmap="Blues", xticks_rotation="vertical")
    plt.title(f"Confusion Matrix for {best_result['Model']}")
    plt.show()

    return results

X = df_heavy[['Longitude', 'Latitude','Split1Urea','Split2Urea','Split3Urea']]
y = df_heavy['GrainYield']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=53)

model = LogisticRegression(max_iter=2000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

X = df_heavy.drop(columns=['GrainYield'])
y = df_heavy['GrainYield']

results = evaluate_models(X, y, models)

from sklearn.linear_model import LogisticRegression

X = df_low[['Longitude', 'Latitude','Split1Urea','Split2Urea','Split3Urea']]
y = df_low['GrainYield']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=53)


model = LogisticRegression(max_iter=2000)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

df_low['GrainYield'].value_counts()

X = df_low[['Longitude', 'Latitude','Split1Urea','Split2Urea','Split3Urea']]
y = df_low['GrainYield']

results = evaluate_models(X, y, models)

from sklearn.linear_model import LogisticRegression

X = df_medium[['Longitude', 'Latitude','Split1Urea','Split2Urea','Split3Urea']]
y = df_medium['GrainYield']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=53)


model = LogisticRegression(max_iter=2000)
model.fit(X_train, y_train)


y_pred = model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy}')

df_medium['GrainYield'].value_counts()

X = df_medium.drop(columns=['GrainYield'])
y = df_medium['GrainYield']

results = evaluate_models(X, y, models)

df_heavy = df[df["SoilType_Heavy"] == 1].copy()
df_heavy = df_heavy.drop(columns=['SoilType_Heavy', 'SoilType_Low', 'SoilType_Medium'])


df_low = df[df["SoilType_Low"] == 1].copy()
df_low = df_low.drop(columns=['SoilType_Heavy', 'SoilType_Low', 'SoilType_Medium'])


df_combined = pd.concat([df_heavy, df_low], ignore_index=True)


df_combined = df_combined.drop_duplicates()

X = df_combined.drop(columns=['GrainYield'])
y = df_combined['GrainYield']


results = evaluate_models(X, y, models)

import pandas as pd
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows

output_file = "data.xlsx"
wb = Workbook()
ws = wb.active
ws.title = "Data"

df_heavy_copy = df_heavy.copy()
df_combined_copy = df_combined.copy()
df_medium_copy= df_medium.copy()

ws.append(["Heavy Data"])
for row in dataframe_to_rows(df_heavy_copy, index=False, header=True):
    ws.append(row)

ws.append([])


ws.append(["Low-Heavy Data"])
for row in dataframe_to_rows(df_combined_copy, index=False, header=True):
    ws.append(row)

ws.append([])

ws.append(["Medium Data"])
for row in dataframe_to_rows(df_medium_copy, index=False, header=True):
    ws.append(row)

wb.save(output_file)