# -*- coding: utf-8 -*-
"""SowingSchedule.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FPp2J32lYo4dBRgTfgn_BfW0wQw4U7FB
"""

import numpy as np
import pandas as pd
from sklearn.impute import KNNImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import MinMaxScaler

df=pd.read_excel('Data_processed.xlsx')
df_=pd.read_excel('Original_Data.xls')

missing_values = df.isnull().sum()

missing_categories = missing_values[missing_values > 0]

print("Boş değerleri olan kategoriler:\n", missing_categories)

# Clean up spaces and incorrect characters
df['Longitude'] = df['Longitude'].astype(str)  # Convert data to string
df['Longitude'] = df['Longitude'].str.replace(r'[^\d\.\-]', '', regex=True) # Remove everything other than numbers and periods
df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')  # String to float, make errors NaN

mean_longitude = df['Longitude'].mean()
df['Longitude'].fillna(mean_longitude)

# Fill missing values using the previous valid value
missing_variables = ["HerbicideYear","HerbicideMonth", "HerbicideWeekNum"]
missing_columns_in_dataset = [col for col in df.columns if col in missing_variables]

for col in missing_columns_in_dataset:
    df[col] = df[col].ffill()

# Initialize KNNImputer with 5 nearest neighbors for imputing missing values
missing1_variables =["HerbicideDay","DaysFromSowingToHerbicide","DaysFromHerbicideToHarvest"]
missing1_columns_in_dataset = [col for col in df.columns if col in missing1_variables]

imputer = KNNImputer(n_neighbors=5)
df[missing1_columns_in_dataset] = imputer.fit_transform(df[missing1_columns_in_dataset])

for col in missing1_columns_in_dataset:
    # Convert the column values to numeric, coercing invalid entries to NaN
    df[col] = pd.to_numeric(df[col], errors='coerce')
    # Fill NaN values with 0
    df[col] = df[col].fillna(0).astype(int)

missing_values = df.isnull().sum()

missing_categories = missing_values[missing_values > 0]

print("Boş değerleri olan kategoriler:\n", missing_categories)

df['Longitude'] = df['Longitude'].astype(str)
df['Longitude'] = df['Longitude'].str.replace(r'[^\d\.\-]', '', regex=True)
df['Longitude'] = pd.to_numeric(df['Longitude'], errors='coerce')
mean_longitude = df['Longitude'].mean()
df['Longitude'] = df['Longitude'].fillna(mean_longitude)

missing_values = df.isnull().sum()

missing_categories = missing_values[missing_values > 0]

print("Boş değerleri olan kategoriler:\n", missing_categories)

# To improve performance, we converted everything to numeric and normalized values between 0 and 1
scaler = MinMaxScaler()

columns_to_scale = df.columns[df.columns.get_loc('Longitude'):df.columns.get_loc('DaysFromHerbicideToHarvest') + 1]
df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])

import matplotlib.pyplot as plt

# Count the occurrences of each category in 'LandType'
sowingschedule_counts = df_['SowingSchedule'].value_counts()

# Plot the distribution as a bar chart
sowingschedule_counts.plot(kind='bar')
plt.title("Distribution of SowingScheduleType")
plt.xlabel("SowingSchedule Categories")
plt.ylabel("Count")
plt.show()

df['GrainYield']=df['GrainYield'].map({'A':0,'B':1,'C':2})
Y = df['GrainYield'].to_numpy()

for col in df.columns:
    # Eğer sütun sayısal değilse
    if df[col].dtype != 'int64' and df[col].dtype != 'float64':
        # Sütunu sayısal hale getirmeye çalışın, hataları NaN olarak değiştirin
        df[col] = pd.to_numeric(df[col], errors='coerce')

df

# T1 kategorisi
df_t1 = df[df["SowingSchedule_T1"] == 1].copy()
df_t1 = df_t1.drop(columns=['SowingSchedule_T1', 'SowingSchedule_T2', 'SowingSchedule_T3', 'SowingSchedule_T4', 'SowingSchedule_T5'])
df_t1['SowingSchedule'] = 'T1'

# T2 kategorisi
df_t2 = df[df["SowingSchedule_T2"] == 1].copy()
df_t2 = df_t2.drop(columns=['SowingSchedule_T1', 'SowingSchedule_T2', 'SowingSchedule_T3', 'SowingSchedule_T4', 'SowingSchedule_T5'])
df_t2['SowingSchedule'] = 'T2'

# T3 kategorisi
df_t3 = df[df["SowingSchedule_T3"] == 1].copy()
df_t3 = df_t3.drop(columns=['SowingSchedule_T1', 'SowingSchedule_T2', 'SowingSchedule_T3', 'SowingSchedule_T4', 'SowingSchedule_T5'])
df_t3['SowingSchedule'] = 'T3'

# T4 kategorisi
df_t4 = df[df["SowingSchedule_T4"] == 1].copy()
df_t4 = df_t4.drop(columns=['SowingSchedule_T1', 'SowingSchedule_T2', 'SowingSchedule_T3', 'SowingSchedule_T4', 'SowingSchedule_T5'])
df_t4['SowingSchedule'] = 'T4'

# T5 kategorisi
df_t5 = df[df["SowingSchedule_T5"] == 1].copy()
df_t5 = df_t5.drop(columns=['SowingSchedule_T1', 'SowingSchedule_T2', 'SowingSchedule_T3', 'SowingSchedule_T4', 'SowingSchedule_T5'])
df_t5['SowingSchedule'] = 'T5'

print("T1 rows:", df_t1.shape)
print("T2 rows:", df_t2.shape)
print("T3 rows:", df_t3.shape)
print("T4 rows:", df_t4.shape)
print("T5 rows:", df_t5.shape)

# Birleştirilmiş DataFrame
df_combined = pd.concat([df_t1, df_t2, df_t3, df_t4, df_t5])

# SowingSchedule dağılımını görselleştirme
df_combined['SowingSchedule'].value_counts().plot(kind='bar')
plt.title("Distribution of SowingScheduleType")
plt.xlabel("SowingSchedule Categories")
plt.ylabel("Count")
plt.show()

datasets = [df_t1, df_t2, df_t3, df_t4, df_t5]

for df in datasets:
    missing_values = df.isnull().sum()
    missing_categories = missing_values[missing_values > 0]
    print("Boş değerleri olan kategoriler:\n", missing_categories)

import pandas as pd
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows

output_file = "data.xlsx"
wb = Workbook()
ws = wb.active
ws.title = "Data"

df_combined_copy = df_combined.copy()
df_t1_copy = df_t1.copy()
df_t2_copy = df_t2.copy()
df_t3_copy = df_t3.copy()
df_t4_copy = df_t4.copy()
df_t5_copy = df_t5.copy()

ws.append(["T1 Data"])
for row in dataframe_to_rows(df_t1_copy, index=False, header=True):
    ws.append(row)

ws.append([])

ws.append(["T2 Data"])
for row in dataframe_to_rows(df_t2_copy, index=False, header=True):
    ws.append(row)

ws.append([])

ws.append(["T3 Data"])
for row in dataframe_to_rows(df_t3_copy, index=False, header=True):
    ws.append(row)

ws.append([])

ws.append(["T4 Data"])
for row in dataframe_to_rows(df_t4_copy, index=False, header=True):
    ws.append(row)

ws.append([])

ws.append(["T5 Data"])
for row in dataframe_to_rows(df_t5_copy, index=False, header=True):
    ws.append(row)

ws.append([])

wb.save(output_file)

from sklearn.model_selection import StratifiedKFold
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, matthews_corrcoef, roc_auc_score
import numpy as np
from sklearn.feature_selection import f_classif, SelectKBest
def evaluate_model(model, X, y, num_features=5):

    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=53)
    results = []

    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):
        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

        # Özellik seçimi
        f_values, p_values = f_classif(X, y)

        # Rank features based on F-values
        feature_ranks = np.argsort(f_values)[::-1]  # Sort in descending order

        # Select top k features based on rank
        k = 5  # Number of features to select
        selected_features = X.columns[feature_ranks[:k]]

        X_train_selected = X_train[selected_features]
        X_val_selected = X_val[selected_features]

        # Modelin eğitilmesi
        model.fit(X_train_selected, y_train)
        y_pred = model.predict(X_val_selected)


        # AUC hesabı için olasılıklar
        try:
            y_prob = model.predict_proba(X_val_selected)
            auc = roc_auc_score(y_val, y_prob, multi_class='ovr', average='weighted')
        except AttributeError:
            auc = None

        accuracy = accuracy_score(y_val, y_pred)
        f1 = f1_score(y_val, y_pred, average='weighted')
        precision = precision_score(y_val, y_pred, average='weighted')
        recall = recall_score(y_val, y_pred, average='weighted')
        mcc = matthews_corrcoef(y_val, y_pred)


        results.append({
            'Selected Features': selected_features,
            'Fold': fold + 1,
            'Accuracy': accuracy,
            'AUC': auc,
            'F1 Score': f1,
            'Precision': precision,
            'Recall': recall,
            'MCC': mcc,
        })

        avg_results = {
            'Accuracy': sum(r['Accuracy'] for r in results) / len(results),
            'AUC': sum(r['AUC'] for r in results) / len(results) if all(r['AUC'] is not None for r in results) else None,
            'F1 Score': sum(r['F1 Score'] for r in results) / len(results),
            'Precision': sum(r['Precision'] for r in results) / len(results),
            'Recall': sum(r['Recall'] for r in results) / len(results),
            'MCC': sum(r['MCC'] for r in results) / len(results),
        }

        return avg_results, results

import xgboost as xgb
import lightgbm as lgb
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier # Import necessary classes
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC

models = [
    ('Decision Tree', DecisionTreeClassifier(max_depth=8,random_state=53)),
    ('Random Forest', RandomForestClassifier(n_estimators=50,bootstrap=True,random_state=53)),
    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=50, subsample=0.8, random_state=53)),
    ('AdaBoost', AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=53)),
    ('Extra Trees', ExtraTreesClassifier(n_estimators=100, max_depth=8, random_state=53)),
    ('SVC', SVC(C=0.5,kernel='linear', random_state=53)),

]

from sklearn.model_selection import GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 20],
    'min_samples_split': [2, 5, 10]
}

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score
from imblearn.over_sampling import SMOTE
# Label Encoder tanımla
le = LabelEncoder()

# Datasetleri işle
for df in datasets:
    # Kategorik sütunları sayısal değerlere çevir
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = le.fit_transform(df[col])

    # Features (X) ve Target (Y)
    X = df.drop(columns=['GrainYield'])  # Hedef sütunu çıkar
    Y = df['GrainYield'].to_numpy()

    # Train-test split
    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

    smote = SMOTE()
    X_train_balanced, Y_train_balanced = smote.fit_resample(X_train, Y_train)

    # Modelleri sırayla çalıştır
    for model_name, model in models:
        # Model eğitimi
        model.fit(X_train, Y_train)

        # Tahmin
        predictions = model.predict(X_test)

        # Performans değerlendirmesi
        accuracy = accuracy_score(Y_test, predictions)
        f1 = f1_score(Y_test, predictions, average='weighted')
        precision = precision_score(Y_test, predictions, average='weighted')
        recall = recall_score(Y_test, predictions, average='weighted')

        # Sonuçları yazdır
        print(f"Model: {model_name}")
        print(f"  Accuracy: {accuracy:.4f}")
        print(f"  F1 Score: {f1:.4f}")
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall: {recall:.4f}")
        print("-" * 30)

all_top_features = []  # Tüm datasetlerden gelen önemli özellikler burada birikecek

datasets = [
    ("SowingSchedule_T1 Dataset", df_t1),
    ("SowingSchedule_T2 Dataset", df_t2),
    ("SowingSchedule_T3 Dataset", df_t3),
    ("SowingSchedule_T4 Dataset", df_t4),
    ("SowingSchedule_T5 Dataset", df_t5)
]
for df in datasets:
    dataset_name, df = df
    print(f"Processing {dataset_name}")
    # Bağımsız ve bağımlı değişkenleri tanımlayın
    X = df[top_features['Feature']]  # Önemli özellikleri alın
    y = df['GrainYield']  # Hedef değişken

    dataset_top_features = []  # Sadece bu dataset için önemli özellikler

    for model_name, model in models:
        if hasattr(model, 'feature_importances_'):  # Model feature_importances_ destekliyor mu kontrol edin
            model.fit(X, y)  # Modeli eğit
            feature_importances = model.feature_importances_

            # Özellik uzunluğu eşleşmiyorsa düzeltin
            if len(X.columns) != len(feature_importances):
                print(f"Feature length mismatch in {model_name}. Adjusting...")
                selected_features = X.columns[:len(feature_importances)]
            else:
                selected_features = X.columns

            # Feature importance'ları DataFrame'e dönüştürün
            importance_df = pd.DataFrame({'Feature': selected_features, 'Importance': feature_importances})
            importance_df = importance_df.sort_values(by='Importance', ascending=False)

            if not importance_df.empty:
                top_features = importance_df.head(30)  # İlk 30 özelliği alın
                dataset_top_features.extend(top_features['Feature'].tolist())  # Dataset'e özel listeye ekleyin
            else:
                print(f"No features to add for {model_name}")
        else:
            print(f"{model_name} does not have feature_importances_.")

    # Her bir datasetin önemli özelliklerini toplayın
    print(f"Top features for {dataset_name}:")
    print(", ".join([feature for feature, count in Counter(dataset_top_features).most_common(5)]))
    print("-" * 50)
    all_top_features.extend(dataset_top_features)

from collections import Counter
feature_counts = Counter(all_top_features)
top_features_final = [feature for feature, count in feature_counts.most_common(40)]

print(f"Top 40 Features across all models: {top_features_final}")

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.impute import SimpleImputer
from collections import Counter

# Tüm veri setleri için önemli özellikleri toplamak için liste
all_top_features = []

le = LabelEncoder()
i = 1

for dataset_name, df in datasets:  # Dataset isimlerini ve verilerini al
    print(f"For Dataset {dataset_name}:")  # Dataset adını yazdır
    X = df[top_features['Feature']]  # Bağımsız değişkenler
    y = df['GrainYield']  # Bağımlı değişken

    # Veriyi eğitim ve test setlerine ayır
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=53)

    imputer = SimpleImputer(strategy='mean')  # Eksik değerleri doldur
    X_train = imputer.fit_transform(X_train)
    X_test = imputer.transform(X_test)

    dataset_top_features = []  # Sadece bu dataset için önemli özellikler

    for name, model in models:
        model.fit(X_train, y_train)  # Modeli eğit
        predictions = model.predict(X_test)  # Tahmin yap
        acc = accuracy_score(y_test, predictions)  # Doğruluk skoru hesapla
        print(f"{name} Accuracy: {acc:.4f}")

        # Eğer model `feature_importances_` destekliyorsa, önemli özellikleri yazdır
        if hasattr(model, 'feature_importances_'):
            feature_importances = model.feature_importances_
            selected_features = X.columns[:len(feature_importances)]
            importance_df = pd.DataFrame({'Feature': selected_features, 'Importance': feature_importances})
            importance_df = importance_df.sort_values(by='Importance', ascending=False)

            # En önemli özellikleri dataset'e özel listeye ekle
            top_features = importance_df.head(10)  # İlk 10 özelliği al
            dataset_top_features.extend(top_features['Feature'].tolist())

    # Her dataset için en sık kullanılan özellikleri yazdır
    feature_counts = Counter(dataset_top_features)
    top_features_final = [feature for feature, count in feature_counts.most_common(10)]
    print(f"Top features for {dataset_name}: {', '.join(top_features_final)}")
    print("-" * 50)

    # Tüm datasetlerin önemli özelliklerini birleştir
    all_top_features.extend(dataset_top_features)

# Genel olarak en sık kullanılan özellikleri yazdır
overall_top_features = Counter(all_top_features).most_common(20)
print(f"Top 20 Features across all datasets:")
print(", ".join([feature for feature, count in overall_top_features]))

from sklearn.linear_model import LogisticRegression, SGDClassifier, PassiveAggressiveClassifier, RidgeClassifier
from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB
from sklearn.svm import SVC, LinearSVC, NuSVC
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import StratifiedKFold
import warnings

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)

le = LabelEncoder()
i = 1

for df in datasets:
    print(f"For Dataset {dataset_name}:")

    print(f"For Dataset {i}:")
    i += 1

    models = [
        ('Decision Tree', DecisionTreeClassifier(max_depth=8, random_state=53)),
        ('Random Forest', RandomForestClassifier(n_estimators=50, bootstrap=True, random_state=53)),
        ('Gradient Boosting', GradientBoostingClassifier(n_estimators=50, subsample=0.8, random_state=53)),
        ('AdaBoost', AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=53)),
        ('Bagging Classifier', BaggingClassifier()),
        ('Extra Trees', ExtraTreesClassifier(n_estimators=100, max_depth=8, random_state=53)),
        ('K Neighbors', KNeighborsClassifier()),
        ('Radius Neighbors', RadiusNeighborsClassifier(radius=5)),
        ('SVC', SVC(probability=True)),
        ('Voting Classifier', VotingClassifier(estimators=[
            ('rf', RandomForestClassifier(n_estimators=50, random_state=53)),
            ('gb', GradientBoostingClassifier(n_estimators=50, random_state=53)),
            ('svc', SVC(probability=True, random_state=53))
        ], voting='soft'))
    ]

    # Modelleri sırayla çalıştır
    for model_name, model in models:
        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=53)
        fold_results = []

        for fold, (train_idx, test_idx) in enumerate(skf.split(X, y), start=1):
            # Veriyi eğitim ve test setine ayır
            X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]
            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

            # Modeli eğit ve tahmin yap
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)

            # Performans değerlendirmesi
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_test, y_pred, average='weighted')
            f1 = f1_score(y_test, y_pred, average='weighted')

            fold_results.append((accuracy, precision, recall, f1))

        # Katmanlar arası ortalama performans
        avg_accuracy = sum([res[0] for res in fold_results]) / len(fold_results)
        avg_precision = sum([res[1] for res in fold_results]) / len(fold_results)
        avg_recall = sum([res[2] for res in fold_results]) / len(fold_results)
        avg_f1 = sum([res[3] for res in fold_results]) / len(fold_results)

        # Sonuçları yazdır
        print(f"Model: {model_name}")
        print(f"  Average Accuracy: {avg_accuracy:.4f}")
        print(f"  Average Precision: {avg_precision:.4f}")
        print(f"  Average Recall: {avg_recall:.4f}")
        print(f"  Average F1 Score: {avg_f1:.4f}")
        print("-" * 30)
    print("\n")

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier
from sklearn.svm import SVC

# Modeller
models = [
    ('Decision Tree', DecisionTreeClassifier(max_depth=8, random_state=53)),
    ('Random Forest', RandomForestClassifier(n_estimators=50, bootstrap=True, random_state=53)),
    ('Gradient Boosting', GradientBoostingClassifier(n_estimators=50, subsample=0.8, random_state=53)),
    ('AdaBoost', AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=53)),
    ('Bagging Classifier', BaggingClassifier()),
    ('Extra Trees', ExtraTreesClassifier(n_estimators=100, max_depth=8, random_state=53)),
    ('K Neighbors', KNeighborsClassifier()),
    ('Radius Neighbors', RadiusNeighborsClassifier(radius=5)),
    ('SVC', SVC(probability=True))  # ROC için probability=True ekliyoruz
]

for df in datasets:
  # Örnek veri seti
  from sklearn.datasets import make_classification
  X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

  # Eğitim ve test setine ayırma
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

  # ROC grafiği çizimi
  plt.figure(figsize=(12, 8))

  for model_name, model in models:
      # Model eğitimi
      model.fit(X_train, y_train)

      # Tahmin edilen olasılıklar veya decision scores
      if hasattr(model, "predict_proba"):
          y_scores = model.predict_proba(X_test)[:, 1]
      elif hasattr(model, "decision_function"):
          y_scores = model.decision_function(X_test)
      else:
          print(f"{model_name} için ROC eğrisi çizilemiyor.")
          continue

      # ROC eğrisi için TPR ve FPR
      fpr, tpr, thresholds = roc_curve(y_test, y_scores)
      auc = roc_auc_score(y_test, y_scores)

      # ROC eğrisini çiz
      plt.plot(fpr, tpr, label=f"{model_name} (AUC = {auc:.2f})")

  # Rastgele tahmin çizgisi
  plt.plot([0, 1], [0, 1], 'k--', label="Random Guessing (AUC = 0.50)")

  # Grafik düzenlemesi
  plt.xlabel("False Positive Rate")
  plt.ylabel("True Positive Rate")
  plt.title("ROC Curve Comparison")
  plt.legend(loc="lower right")
  plt.grid()
  plt.show()
  print(" \n")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score
import numpy as np

# En iyi model için değerlendirme
best_model_name = None
best_model = None
best_accuracy = 0
best_predictions = None

for df in datasets:
  # Modelleri eğit ve doğruluklarını hesapla
  for model_name, model in models:
      # Model eğitimi
      model.fit(X_train, y_train)

      # Tahmin yap
      predictions = model.predict(X_test)

      # Doğruluk hesapla
      acc = accuracy_score(y_test, predictions)

      if acc > best_accuracy:
          best_accuracy = acc
          best_model_name = model_name
          best_model = model
          best_predictions = predictions

  # En iyi modelin Confusion Matrix'ini oluştur
  print(f"Best Model: {best_model_name} with Accuracy: {best_accuracy:.4f}")
  cm = confusion_matrix(y_test, best_predictions)

  # Confusion Matrix'i görselleştir
  disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.unique(y))
  disp.plot(cmap="Blues")
  plt.title(f"Confusion Matrix for {best_model_name}")
  plt.show()